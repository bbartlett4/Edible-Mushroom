{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "how-to-balance-a-dataset-f1-score-88.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bbartlett4/Edible-Mushroom/blob/main/how_to_balance_a_dataset_f1_score_88.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **This notebook contains a custom callback you may wish to copy and use  \n",
        "It is a combination of the Keras callbacks Reduce Learning Rate on Plateau,  \n",
        "Early Stopping and Model Checkpoint but eliminates some of the limitations  \n",
        "of each. In addition it provides an easier to read summary of the model's  \n",
        "performance at the end of each epoch. It also provides a handy feature  \n",
        "that enables you to set the number of epochs to train for until a message  \n",
        "asks if you wish to halt training on the current epoch by entering H or  \n",
        "to enter an integer which will determine how many more epochs to run  \n",
        "before the message appears again. This is very useful if you are training  \n",
        "a model and decide the metrics are satisfactory and you want to end  \n",
        "the model training early. Note the callback always returns your model  \n",
        "with the weights set to those of the epoch which had the highest performance  \n",
        "on the metric being monitored (accuracy or validation accuracy)  \n",
        "The callback initially monitors training accuracy and will adjust the learning  \n",
        "rate based on that until the accuracy reaches a user specified threshold  \n",
        "level. Once that level of training accuracy is achieved the callback switches  \n",
        "to monitoring validation loss and adjusts the learning rate based on that.  \n",
        "the callback is of the form:  \n",
        "callbacks=[LRA(model, base_model, patience, stop_patience, threshold,factor, dwell,\n",
        "              batches, initial_epoch, epochs, ask_epoch )]    \n",
        " **where:**\n",
        " - **model** is your compiled model  \n",
        " - **base_model** is the name of your base_model if you are doing transfer learning.  \n",
        "      for example you might have in your model  \n",
        "      base_model=tf.keras.applications.EfficientNetB1(include_top=False, weights=\"imagenet\",input_shape=img_shape, pooling='max')\n",
        "      base_model.trainabel=False   During training you will be asked if you want to do fine tuning  \n",
        "      If you enter F to the query, the base_model will be set to trainable by the callback\n",
        "      If you are not doing transfer learning set base_model==None\n",
        " - **patience** is an integer that determines many consecutive epochs can occur before the learning rate\n",
        "   will be adjusted (similar to patience parameter in Reduce Learning Rate on Plateau)\n",
        " \n",
        " - **stop_patience** is an integer that determines hom many consecutive epochs for which the\n",
        "   learning rate was adjusted but no improvement in the monitored metric occurred before\n",
        "   training is halted(similar to patience parameter in early stopping)\n",
        " \n",
        " - **threshold** is a float that determines the level that training accuracy must achieve\n",
        "   before the callback switches over to monitoring validation loss. This  is useful for\n",
        "   cases where the validation loss in early epochs tends to vary widely and can cause\n",
        "   unwanted behavior when using the conventional Keras callbacks\n",
        " - **factor** is a float that determines the new learning rate by the equation lr=lr*factor.\n",
        "   (similar to the factor parameter in Reduce Learning Rate on Plateau)\n",
        " - **dwell** is a boolean. It is used in the callback as part of an experiment on training\n",
        "   models. If on a given epoch the metric being monitored fails to improve it means\n",
        "   your model has moved to a location on the surface of Nspace (where N is the number\n",
        "   of trainable parameters) that is NOT as favorable (poorer metric performance) than\n",
        "   the position in Nspace you were in for the previous epoch. If dwell is set to True\n",
        "   the callback loads the model with the weights from the previous (better metric value)\n",
        "   epoch. Why move to a worse place if the place you were in previously was better. Then\n",
        "   the learning rate is reduced for the next epoch of training. If dwell is set to false\n",
        "   this action does not take place.\n",
        " - **batches** is an integer. It should be set to a value of \n",
        "   batches=int(number of traing samples/batch_size). During training the callback provides\n",
        "   information during an epoch of the form\n",
        "   'processing batch of batches  accuracy= accuracy  loss= loss where batch is the current \n",
        "    batch being processs, batches is as described above, accuracy is the current training\n",
        "    accuracy and loss is the current loss. Typically the message would appear as\n",
        "    processing batch 25 of 50  accuracy: 54%  loss: .04567. As each batch is processed\n",
        "    these values are changed.    \n",
        " - **initial_epoch** is an integer. Typically set this to zero Itis used in the information\n",
        "    printed out for each epoch. In the case where you train the model say with the\n",
        "    basemodel weights frozen say you train for 10 epochs. Then you want to fine tune\n",
        "    the model and train for more eppochs for the second training session you would\n",
        "    reinstantiate the callback and set initial_epoch=10.\n",
        " - **epochs** an integer value for the number of epochs to train\n",
        " - **ask_epoch** is either set to an integer value or None. If set to an integer it denotes\n",
        "    the epoch number at which user input is requested. If the user enter H training is\n",
        "    halted. If the user inters an integer it represents how many more epochs to run\n",
        "    before you are asked for the user input again. If the user enters F the base_model\n",
        "    is made trainable If ask_epoch is set to None the\n",
        "    user is NOT asked to provide any input. This feature is handy is when training your model\n",
        "    and the metrics are either unsatisfactory and you want to stop training, or for the case\n",
        "    where your metrics are satisfactory and there is no need to train any further. Note\n",
        "    you model is always set to the weights for the epoch that had the beset metric\n",
        "    performance. So if you halt the training you can still use the model for predictions.  \n",
        "      \n",
        "### ** Example of Use:\n",
        " callbacks=[LRA(model=my_model, base_model=base_model, patience=1,stop_patience=3,  \n",
        "            threshold=.9, factor=.5, dwell=True,batches=85, initial_epoch=0 , epochs=20, ask_epoch=5)]\n",
        " this implies:\n",
        " - your model is my_model\n",
        " - base_model is the name of your base_model if you are doing transfer learning\n",
        " - after 1 epoch of no improvement the learning rate will be reduced\n",
        " - after 3 consecutive adjustment of the leaarning rate with no metric improve training terminates\n",
        " - once the training accuracy reaches 90% the callback adjust learning rate based on validation loss\n",
        " - when the learning rate is adjust the new learning rate is .5 X learning rate\n",
        " - if the current epoch's metric value did not improve, the weights for the prior epoch are loaded\n",
        "   and the learning rate is reduced\n",
        " - 85 batches of data are run to complete an epoch \n",
        " - the initial epoch is 0\n",
        " - train for 20 epochs\n",
        " - after the fifth epoch you will be asked if you want to halt training by entering H or enter \n",
        "   an integer denoting how many more epochs to run before you will be prompted again or enter\n",
        "   F to make the base_model=trainable"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.019813,
          "end_time": "2021-05-21T20:58:39.646456",
          "exception": false,
          "start_time": "2021-05-21T20:58:39.626643",
          "status": "completed"
        },
        "tags": [],
        "id": "53wzzYbYS2Hz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import Dense, Activation,Dropout,Conv2D, MaxPooling2D,BatchNormalization, Flatten\n",
        "from tensorflow.keras.optimizers import Adam, Adamax\n",
        "from tensorflow.keras.metrics import categorical_crossentropy\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Model, load_model, Sequential\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import shutil\n",
        "import time\n",
        "import cv2 as cv2\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import imshow\n",
        "import os\n",
        "import seaborn as sns\n",
        "sns.set_style('darkgrid')\n",
        "from PIL import Image\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from IPython.core.display import display, HTML\n",
        "# stop annoying tensorflow warning messages\n",
        "import logging\n",
        "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)"
      ],
      "metadata": {
        "papermill": {
          "duration": 5.693249,
          "end_time": "2021-05-21T20:58:45.358994",
          "exception": false,
          "start_time": "2021-05-21T20:58:39.665745",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-08-09T22:07:59.364193Z",
          "iopub.execute_input": "2021-08-09T22:07:59.364547Z",
          "iopub.status.idle": "2021-08-09T22:07:59.373047Z",
          "shell.execute_reply.started": "2021-08-09T22:07:59.364518Z",
          "shell.execute_reply": "2021-08-09T22:07:59.372086Z"
        },
        "trusted": true,
        "id": "n5pXiZjGS2H7"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define a function to show example training images"
      ],
      "metadata": {
        "id": "31NIb2-pS2H8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_image_samples(gen ):\n",
        "    t_dict=gen.class_indices\n",
        "    classes=list(t_dict.keys())    \n",
        "    images,labels=next(gen) # get a sample batch from the generator \n",
        "    plt.figure(figsize=(20, 20))\n",
        "    length=len(labels)\n",
        "    if length<25:   #show maximum of 25 images\n",
        "        r=length\n",
        "    else:\n",
        "        r=25\n",
        "    for i in range(r):\n",
        "        plt.subplot(5, 5, i + 1)\n",
        "        image=images[i]/255\n",
        "        plt.imshow(image)\n",
        "        index=np.argmax(labels[i])\n",
        "        class_name=classes[index]\n",
        "        plt.title(class_name, color='blue', fontsize=12)\n",
        "        plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.03267,
          "end_time": "2021-05-21T20:59:58.764761",
          "exception": false,
          "start_time": "2021-05-21T20:59:58.732091",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-08-09T22:07:59.426817Z",
          "iopub.execute_input": "2021-08-09T22:07:59.427123Z",
          "iopub.status.idle": "2021-08-09T22:07:59.434414Z",
          "shell.execute_reply.started": "2021-08-09T22:07:59.427095Z",
          "shell.execute_reply": "2021-08-09T22:07:59.433329Z"
        },
        "trusted": true,
        "id": "NfmjVZDRS2H9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_images(tdir):\n",
        "    classlist=os.listdir(tdir)\n",
        "    length=len(classlist)\n",
        "    columns=5\n",
        "    rows=int(np.ceil(length/columns))    \n",
        "    plt.figure(figsize=(20, rows * 4))\n",
        "    for i, klass in enumerate(classlist):    \n",
        "        classpath=os.path.join(tdir, klass)\n",
        "        imgpath=os.path.join(classpath, '1.jpg')\n",
        "        img=plt.imread(imgpath)\n",
        "        plt.subplot(rows, columns, i+1)\n",
        "        plt.axis('off')\n",
        "        plt.title(klass, color='blue', fontsize=12)\n",
        "        plt.imshow(img)\n",
        "    "
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-08-09T22:07:59.480291Z",
          "iopub.execute_input": "2021-08-09T22:07:59.480592Z",
          "iopub.status.idle": "2021-08-09T22:07:59.487471Z",
          "shell.execute_reply.started": "2021-08-09T22:07:59.480566Z",
          "shell.execute_reply": "2021-08-09T22:07:59.486614Z"
        },
        "trusted": true,
        "id": "sCiU1sKqS2H9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define  a function to print text in RGB foreground and background colors"
      ],
      "metadata": {
        "id": "SwTQ1il4S2H9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_in_color(txt_msg,fore_tupple,back_tupple,):\n",
        "    #prints the text_msg in the foreground color specified by fore_tupple with the background specified by back_tupple \n",
        "    #text_msg is the text, fore_tupple is foregroud color tupple (r,g,b), back_tupple is background tupple (r,g,b)\n",
        "    rf,gf,bf=fore_tupple\n",
        "    rb,gb,bb=back_tupple\n",
        "    msg='{0}' + txt_msg\n",
        "    mat='\\33[38;2;' + str(rf) +';' + str(gf) + ';' + str(bf) + ';48;2;' + str(rb) + ';' +str(gb) + ';' + str(bb) +'m' \n",
        "    print(msg .format(mat), flush=True)\n",
        "    print('\\33[0m', flush=True) # returns default print color to back to black\n",
        "    return"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.078026,
          "end_time": "2021-05-21T21:00:02.099684",
          "exception": false,
          "start_time": "2021-05-21T21:00:02.021658",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-08-09T22:07:59.54181Z",
          "iopub.execute_input": "2021-08-09T22:07:59.542123Z",
          "iopub.status.idle": "2021-08-09T22:07:59.548899Z",
          "shell.execute_reply.started": "2021-08-09T22:07:59.542094Z",
          "shell.execute_reply": "2021-08-09T22:07:59.547997Z"
        },
        "trusted": true,
        "id": "XlPnLlicS2H-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define a subclass of Keras callbacks that will control the learning rate and print\n",
        "### training data in spreadsheet format. The callback also includes a feature to\n",
        "### periodically ask if you want to train for N more epochs or halt"
      ],
      "metadata": {
        "id": "BGoFIJt8S2H-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LRA(keras.callbacks.Callback):\n",
        "    def __init__(self,model, base_model, patience,stop_patience, threshold, factor, dwell, batches, initial_epoch,epochs, ask_epoch):\n",
        "        super(LRA, self).__init__()\n",
        "        self.model=model\n",
        "        self.base_model=base_model\n",
        "        self.patience=patience # specifies how many epochs without improvement before learning rate is adjusted\n",
        "        self.stop_patience=stop_patience # specifies how many times to adjust lr without improvement to stop training\n",
        "        self.threshold=threshold # specifies training accuracy threshold when lr will be adjusted based on validation loss\n",
        "        self.factor=factor # factor by which to reduce the learning rate\n",
        "        self.dwell=dwell\n",
        "        self.batches=batches # number of training batch to runn per epoch\n",
        "        self.initial_epoch=initial_epoch\n",
        "        self.epochs=epochs\n",
        "        self.ask_epoch=ask_epoch\n",
        "        self.ask_epoch_initial=ask_epoch # save this value to restore if restarting training\n",
        "        # callback variables \n",
        "        self.count=0 # how many times lr has been reduced without improvement\n",
        "        self.stop_count=0        \n",
        "        self.best_epoch=1   # epoch with the lowest loss        \n",
        "        self.initial_lr=float(tf.keras.backend.get_value(model.optimizer.lr)) # get the initiallearning rate and save it         \n",
        "        self.highest_tracc=0.0 # set highest training accuracy to 0 initially\n",
        "        self.lowest_vloss=np.inf # set lowest validation loss to infinity initially\n",
        "        self.best_weights=self.model.get_weights() # set best weights to model's initial weights\n",
        "        self.initial_weights=self.model.get_weights()   # save initial weights if they have to get restored \n",
        "        \n",
        "    def on_train_begin(self, logs=None):        \n",
        "        if self.base_model != None:\n",
        "            status=base_model.trainable\n",
        "            if status:\n",
        "                msg=' initializing callback starting train with base_model trainable'\n",
        "            else:\n",
        "                msg='initializing callback starting training with base_model not trainable'\n",
        "        else:\n",
        "            msg='initialing callback and starting training'                        \n",
        "        print_in_color (msg, (244, 252, 3), (55,65,80)) \n",
        "        msg='{0:^8s}{1:^10s}{2:^9s}{3:^9s}{4:^9s}{5:^9s}{6:^9s}{7:^10s}{8:10s}{9:^8s}'.format('Epoch', 'Loss', 'Accuracy',\n",
        "                                                                                              'V_loss','V_acc', 'LR', 'Next LR', 'Monitor','% Improv', 'Duration')\n",
        "        print_in_color(msg, (244,252,3), (55,65,80)) \n",
        "        self.start_time= time.time()\n",
        "        \n",
        "    def on_train_end(self, logs=None):\n",
        "        stop_time=time.time()\n",
        "        tr_duration= stop_time- self.start_time            \n",
        "        hours = tr_duration // 3600\n",
        "        minutes = (tr_duration - (hours * 3600)) // 60\n",
        "        seconds = tr_duration - ((hours * 3600) + (minutes * 60))\n",
        "\n",
        "        self.model.set_weights(self.best_weights) # set the weights of the model to the best weights\n",
        "        msg=f'Training is completed - model is set with weights from epoch {self.best_epoch} '\n",
        "        print_in_color(msg, (0,255,0), (55,65,80))\n",
        "        msg = f'training elapsed time was {str(hours)} hours, {minutes:4.1f} minutes, {seconds:4.2f} seconds)'\n",
        "        print_in_color(msg, (0,255,0), (55,65,80))   \n",
        "        \n",
        "    def on_train_batch_end(self, batch, logs=None):\n",
        "        acc=logs.get('accuracy')* 100  # get training accuracy \n",
        "        loss=logs.get('loss')\n",
        "        msg='{0:20s}processing batch {1:4s} of {2:5s} accuracy= {3:8.3f}  loss: {4:8.5f}'.format(' ', str(batch), str(self.batches), acc, loss)\n",
        "        print(msg, '\\r', end='') # prints over on the same line to show running batch count        \n",
        "        \n",
        "    def on_epoch_begin(self,epoch, logs=None):\n",
        "        self.now= time.time()\n",
        "        \n",
        "    def on_epoch_end(self, epoch, logs=None):  # method runs on the end of each epoch\n",
        "        later=time.time()\n",
        "        duration=later-self.now \n",
        "        lr=float(tf.keras.backend.get_value(self.model.optimizer.lr)) # get the current learning rate\n",
        "        current_lr=lr\n",
        "        v_loss=logs.get('val_loss')  # get the validation loss for this epoch\n",
        "        acc=logs.get('accuracy')  # get training accuracy \n",
        "        v_acc=logs.get('val_accuracy')\n",
        "        loss=logs.get('loss')        \n",
        "        if acc < self.threshold: # if training accuracy is below threshold adjust lr based on training accuracy\n",
        "            monitor='accuracy'\n",
        "            if epoch ==0:\n",
        "                pimprov=0.0\n",
        "            else:\n",
        "                pimprov= (acc-self.highest_tracc )*100/self.highest_tracc\n",
        "            if acc>self.highest_tracc: # training accuracy improved in the epoch                \n",
        "                self.highest_tracc=acc # set new highest training accuracy\n",
        "                self.best_weights=self.model.get_weights() # traing accuracy improved so save the weights\n",
        "                self.count=0 # set count to 0 since training accuracy improved\n",
        "                self.stop_count=0 # set stop counter to 0\n",
        "                if v_loss<self.lowest_vloss:\n",
        "                    self.lowest_vloss=v_loss\n",
        "                color= (0,255,0)\n",
        "                self.best_epoch=epoch + 1  # set the value of best epoch for this epoch              \n",
        "            else: \n",
        "                # training accuracy did not improve check if this has happened for patience number of epochs\n",
        "                # if so adjust learning rate\n",
        "                if self.count>=self.patience -1: # lr should be adjusted\n",
        "                    color=(245, 170, 66)\n",
        "                    lr= lr* self.factor # adjust the learning by factor\n",
        "                    tf.keras.backend.set_value(self.model.optimizer.lr, lr) # set the learning rate in the optimizer\n",
        "                    self.count=0 # reset the count to 0\n",
        "                    self.stop_count=self.stop_count + 1 # count the number of consecutive lr adjustments\n",
        "                    self.count=0 # reset counter\n",
        "                    if self.dwell:\n",
        "                        self.model.set_weights(self.best_weights) # return to better point in N space                        \n",
        "                    else:\n",
        "                        if v_loss<self.lowest_vloss:\n",
        "                            self.lowest_vloss=v_loss                                    \n",
        "                else:\n",
        "                    self.count=self.count +1 # increment patience counter                    \n",
        "        else: # training accuracy is above threshold so adjust learning rate based on validation loss\n",
        "            monitor='val_loss'\n",
        "            if epoch ==0:\n",
        "                pimprov=0.0\n",
        "            else:\n",
        "                pimprov= (self.lowest_vloss- v_loss )*100/self.lowest_vloss\n",
        "            if v_loss< self.lowest_vloss: # check if the validation loss improved \n",
        "                self.lowest_vloss=v_loss # replace lowest validation loss with new validation loss                \n",
        "                self.best_weights=self.model.get_weights() # validation loss improved so save the weights\n",
        "                self.count=0 # reset count since validation loss improved  \n",
        "                self.stop_count=0  \n",
        "                color=(0,255,0)                \n",
        "                self.best_epoch=epoch + 1 # set the value of the best epoch to this epoch\n",
        "            else: # validation loss did not improve\n",
        "                if self.count>=self.patience-1: # need to adjust lr\n",
        "                    color=(245, 170, 66)\n",
        "                    lr=lr * self.factor # adjust the learning rate                    \n",
        "                    self.stop_count=self.stop_count + 1 # increment stop counter because lr was adjusted \n",
        "                    self.count=0 # reset counter\n",
        "                    tf.keras.backend.set_value(self.model.optimizer.lr, lr) # set the learning rate in the optimizer\n",
        "                    if self.dwell:\n",
        "                        self.model.set_weights(self.best_weights) # return to better point in N space\n",
        "                else: \n",
        "                    self.count =self.count +1 # increment the patience counter                    \n",
        "                if acc>self.highest_tracc:\n",
        "                    self.highest_tracc= acc\n",
        "        msg=f'{str(epoch+1):^3s}/{str(self.epochs):4s} {loss:^9.3f}{acc*100:^9.3f}{v_loss:^9.5f}{v_acc*100:^9.3f}{current_lr:^9.5f}{lr:^9.5f}{monitor:^11s}{pimprov:^10.2f}{duration:^8.2f}'\n",
        "        print_in_color (msg,color, (55,65,80))\n",
        "        if self.stop_count> self.stop_patience - 1: # check if learning rate has been adjusted stop_count times with no improvement\n",
        "            msg=f' training has been halted at epoch {epoch + 1} after {self.stop_patience} adjustments of learning rate with no improvement'\n",
        "            print_in_color(msg, (0,255,255), (55,65,80))\n",
        "            self.model.stop_training = True # stop training\n",
        "        else: \n",
        "            if self.ask_epoch !=None:\n",
        "                if epoch + 1 >= self.ask_epoch:\n",
        "                    if base_model.trainable:\n",
        "                        msg='enter H to halt  or an integer for number of epochs to run then ask again'\n",
        "                    else:\n",
        "                        msg='enter H to halt ,F to fine tune model, or an integer for number of epochs to run then ask again'\n",
        "                    print_in_color(msg, (0,255,255), (55,65,80))\n",
        "                    ans=input('')\n",
        "                    if ans=='H' or ans=='h':\n",
        "                        msg=f'training has been halted at epoch {epoch + 1} due to user input'\n",
        "                        print_in_color(msg, (0,255,255), (55,65,80))\n",
        "                        self.model.stop_training = True # stop training\n",
        "                    elif ans == 'F' or ans=='f':\n",
        "                        if base_model.trainable:\n",
        "                            msg='base_model is already set as trainable'\n",
        "                        else:\n",
        "                            msg='setting base_model as trainable for fine tuning of model'\n",
        "                            self.base_model.trainable=True\n",
        "                        print_in_color(msg, (0, 255,255), (55,65,80))\n",
        "                        msg='{0:^8s}{1:^10s}{2:^9s}{3:^9s}{4:^9s}{5:^9s}{6:^9s}{7:^10s}{8:^8s}'.format('Epoch', 'Loss', 'Accuracy',\n",
        "                                                                                              'V_loss','V_acc', 'LR', 'Next LR', 'Monitor','% Improv', 'Duration')\n",
        "                        print_in_color(msg, (244,252,3), (55,65,80))                         \n",
        "                        self.count=0\n",
        "                        self.stop_count=0                        \n",
        "                        self.ask_epoch = epoch + 1 + self.ask_epoch_initial \n",
        "                        \n",
        "                    else:\n",
        "                        ans=int(ans)\n",
        "                        self.ask_epoch +=ans\n",
        "                        msg=f' training will continue until epoch ' + str(self.ask_epoch)                         \n",
        "                        print_in_color(msg, (0, 255,255), (55,65,80))\n",
        "                        msg='{0:^8s}{1:^10s}{2:^9s}{3:^9s}{4:^9s}{5:^9s}{6:^9s}{7:^10s}{8:10s}{9:^8s}'.format('Epoch', 'Loss', 'Accuracy',\n",
        "                                                                                              'V_loss','V_acc', 'LR', 'Next LR', 'Monitor','% Improv', 'Duration')\n",
        "                        print_in_color(msg, (244,252,3), (55,65,80)) "
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-08-09T22:07:59.607134Z",
          "iopub.execute_input": "2021-08-09T22:07:59.607473Z",
          "iopub.status.idle": "2021-08-09T22:07:59.656793Z",
          "shell.execute_reply.started": "2021-08-09T22:07:59.607436Z",
          "shell.execute_reply": "2021-08-09T22:07:59.654022Z"
        },
        "trusted": true,
        "id": "1O1tQP28S2H_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define a function to plot the training data"
      ],
      "metadata": {
        "id": "6tSpr5gjS2IB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tr_plot(tr_data, start_epoch):\n",
        "    #Plot the training and validation data\n",
        "    tacc=tr_data.history['accuracy']\n",
        "    tloss=tr_data.history['loss']\n",
        "    vacc=tr_data.history['val_accuracy']\n",
        "    vloss=tr_data.history['val_loss']\n",
        "    Epoch_count=len(tacc)+ start_epoch\n",
        "    Epochs=[]\n",
        "    for i in range (start_epoch ,Epoch_count):\n",
        "        Epochs.append(i+1)   \n",
        "    index_loss=np.argmin(vloss)#  this is the epoch with the lowest validation loss\n",
        "    val_lowest=vloss[index_loss]\n",
        "    index_acc=np.argmax(vacc)\n",
        "    acc_highest=vacc[index_acc]\n",
        "    plt.style.use('fivethirtyeight')\n",
        "    sc_label='best epoch= '+ str(index_loss+1 +start_epoch)\n",
        "    vc_label='best epoch= '+ str(index_acc + 1+ start_epoch)\n",
        "    fig,axes=plt.subplots(nrows=1, ncols=2, figsize=(20,8))\n",
        "    axes[0].plot(Epochs,tloss, 'r', label='Training loss')\n",
        "    axes[0].plot(Epochs,vloss,'g',label='Validation loss' )\n",
        "    axes[0].scatter(index_loss+1 +start_epoch,val_lowest, s=150, c= 'blue', label=sc_label)\n",
        "    axes[0].set_title('Training and Validation Loss')\n",
        "    axes[0].set_xlabel('Epochs')\n",
        "    axes[0].set_ylabel('Loss')\n",
        "    axes[0].legend()\n",
        "    axes[1].plot (Epochs,tacc,'r',label= 'Training Accuracy')\n",
        "    axes[1].plot (Epochs,vacc,'g',label= 'Validation Accuracy')\n",
        "    axes[1].scatter(index_acc+1 +start_epoch,acc_highest, s=150, c= 'blue', label=vc_label)\n",
        "    axes[1].set_title('Training and Validation Accuracy')\n",
        "    axes[1].set_xlabel('Epochs')\n",
        "    axes[1].set_ylabel('Accuracy')\n",
        "    axes[1].legend()\n",
        "    plt.tight_layout\n",
        "    #plt.style.use('fivethirtyeight')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "papermill": {
          "duration": 3.215798,
          "end_time": "2021-05-21T22:17:13.46258",
          "exception": false,
          "start_time": "2021-05-21T22:17:10.246782",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-08-09T22:07:59.658786Z",
          "iopub.execute_input": "2021-08-09T22:07:59.659342Z",
          "iopub.status.idle": "2021-08-09T22:07:59.67994Z",
          "shell.execute_reply.started": "2021-08-09T22:07:59.659299Z",
          "shell.execute_reply": "2021-08-09T22:07:59.67863Z"
        },
        "trusted": true,
        "id": "alg1pznZS2IC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### define a function to create confusion matrix and classification report"
      ],
      "metadata": {
        "id": "CAY6HGWeS2ID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_info( test_gen, preds, print_code, save_dir, subject ):\n",
        "    class_dict=test_gen.class_indices\n",
        "    labels= test_gen.labels\n",
        "    file_names= test_gen.filenames \n",
        "    error_list=[]\n",
        "    true_class=[]\n",
        "    pred_class=[]\n",
        "    prob_list=[]\n",
        "    new_dict={}\n",
        "    error_indices=[]\n",
        "    y_pred=[]\n",
        "    for key,value in class_dict.items():\n",
        "        new_dict[value]=key             # dictionary {integer of class number: string of class name}\n",
        "    # store new_dict as a text fine in the save_dir\n",
        "    classes=list(new_dict.values())     # list of string of class names     \n",
        "    errors=0      \n",
        "    for i, p in enumerate(preds):\n",
        "        pred_index=np.argmax(p)         \n",
        "        true_index=labels[i]  # labels are integer values\n",
        "        if pred_index != true_index: # a misclassification has occurred\n",
        "            error_list.append(file_names[i])\n",
        "            true_class.append(new_dict[true_index])\n",
        "            pred_class.append(new_dict[pred_index])\n",
        "            prob_list.append(p[pred_index])\n",
        "            error_indices.append(true_index)            \n",
        "            errors=errors + 1\n",
        "        y_pred.append(pred_index)    \n",
        "    if print_code !=0:\n",
        "        if errors>0:\n",
        "            if print_code>errors:\n",
        "                r=errors\n",
        "            else:\n",
        "                r=print_code           \n",
        "            msg='{0:^28s}{1:^28s}{2:^28s}{3:^16s}'.format('Filename', 'Predicted Class' , 'True Class', 'Probability')\n",
        "            print_in_color(msg, (0,255,0),(55,65,80))\n",
        "            for i in range(r):                \n",
        "                split1=os.path.split(error_list[i])                \n",
        "                split2=os.path.split(split1[0])                \n",
        "                fname=split2[1] + '/' + split1[1]\n",
        "                msg='{0:^28s}{1:^28s}{2:^28s}{3:4s}{4:^6.4f}'.format(fname, pred_class[i],true_class[i], ' ', prob_list[i])\n",
        "                print_in_color(msg, (255,255,255), (55,65,60))\n",
        "                #print(error_list[i]  , pred_class[i], true_class[i], prob_list[i])               \n",
        "        else:\n",
        "            msg='With accuracy of 100 % there are no errors to print'\n",
        "            print_in_color(msg, (0,255,0),(55,65,80))\n",
        "    if errors>0:\n",
        "        plot_bar=[]\n",
        "        plot_class=[]\n",
        "        for  key, value in new_dict.items():        \n",
        "            count=error_indices.count(key) \n",
        "            if count!=0:\n",
        "                plot_bar.append(count) # list containg how many times a class c had an error\n",
        "                plot_class.append(value)   # stores the class \n",
        "        fig=plt.figure()\n",
        "        fig.set_figheight(len(plot_class)/3)\n",
        "        fig.set_figwidth(10)\n",
        "        plt.style.use('fivethirtyeight')\n",
        "        for i in range(0, len(plot_class)):\n",
        "            c=plot_class[i]\n",
        "            x=plot_bar[i]\n",
        "            plt.barh(c, x, )\n",
        "            plt.title( ' Errors by Class on Test Set')\n",
        "    y_true= np.array(labels)        \n",
        "    y_pred=np.array(y_pred)\n",
        "    if len(classes)<= 30:\n",
        "        # create a confusion matrix \n",
        "        cm = confusion_matrix(y_true, y_pred )        \n",
        "        length=len(classes)\n",
        "        if length<8:\n",
        "            fig_width=8\n",
        "            fig_height=8\n",
        "        else:\n",
        "            fig_width= int(length * .5)\n",
        "            fig_height= int(length * .5)\n",
        "        plt.figure(figsize=(fig_width, fig_height))\n",
        "        sns.heatmap(cm, annot=True, vmin=0, fmt='g', cmap='Blues', cbar=False)       \n",
        "        plt.xticks(np.arange(length)+.5, classes, rotation= 90)\n",
        "        plt.yticks(np.arange(length)+.5, classes, rotation=0)\n",
        "        plt.xlabel(\"Predicted\")\n",
        "        plt.ylabel(\"Actual\")\n",
        "        plt.title(\"Confusion Matrix\")\n",
        "        plt.show()\n",
        "    clr = classification_report(y_true, y_pred, target_names=classes)\n",
        "    print(\"Classification Report:\\n----------------------\\n\", clr)"
      ],
      "metadata": {
        "papermill": {
          "duration": 3.03999,
          "end_time": "2021-05-21T22:17:25.43058",
          "exception": false,
          "start_time": "2021-05-21T22:17:22.39059",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-08-09T22:07:59.682052Z",
          "iopub.execute_input": "2021-08-09T22:07:59.682472Z",
          "iopub.status.idle": "2021-08-09T22:07:59.710276Z",
          "shell.execute_reply.started": "2021-08-09T22:07:59.682425Z",
          "shell.execute_reply": "2021-08-09T22:07:59.708998Z"
        },
        "trusted": true,
        "id": "TzoWSbnBS2ID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### define a function to save the model and the associated class_dict.csv file"
      ],
      "metadata": {
        "id": "7UcP4UIZS2ID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def saver(save_path, model, model_name, subject, accuracy,img_size, scalar, generator):\n",
        "    # first save the model\n",
        "    save_id=str (model_name +  '-' + subject +'-'+ str(acc)[:str(acc).rfind('.')+3] + '.h5')\n",
        "    model_save_loc=os.path.join(save_path, save_id)\n",
        "    model.save(model_save_loc)\n",
        "    print_in_color ('model was saved as ' + model_save_loc, (0,255,0),(55,65,80)) \n",
        "    # now create the class_df and convert to csv file    \n",
        "    class_dict=generator.class_indices \n",
        "    height=[]\n",
        "    width=[]\n",
        "    scale=[]\n",
        "    for i in range(len(class_dict)):\n",
        "        height.append(img_size[0])\n",
        "        width.append(img_size[1])\n",
        "        scale.append(scalar)\n",
        "    Index_series=pd.Series(list(class_dict.values()), name='class_index')\n",
        "    Class_series=pd.Series(list(class_dict.keys()), name='class') \n",
        "    Height_series=pd.Series(height, name='height')\n",
        "    Width_series=pd.Series(width, name='width')\n",
        "    Scale_series=pd.Series(scale, name='scale by')\n",
        "    class_df=pd.concat([Index_series, Class_series, Height_series, Width_series, Scale_series], axis=1)    \n",
        "    csv_name='class_dict.csv'\n",
        "    csv_save_loc=os.path.join(save_path, csv_name)\n",
        "    class_df.to_csv(csv_save_loc, index=False) \n",
        "    print_in_color ('class csv file was saved as ' + csv_save_loc, (0,255,0),(55,65,80)) \n",
        "    return model_save_loc, csv_save_loc"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-08-09T22:07:59.73022Z",
          "iopub.execute_input": "2021-08-09T22:07:59.730903Z",
          "iopub.status.idle": "2021-08-09T22:07:59.747341Z",
          "shell.execute_reply.started": "2021-08-09T22:07:59.730856Z",
          "shell.execute_reply": "2021-08-09T22:07:59.746379Z"
        },
        "trusted": true,
        "id": "uNd_p0YhS2ID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### define a function that uses the trained model and the\n",
        "### class_dict.csv file to predict images"
      ],
      "metadata": {
        "id": "tL3pcFTkS2IE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predictor(sdir, csv_path,  model_path, crop_image = False):    \n",
        "    # read in the csv file\n",
        "    class_df=pd.read_csv(csv_path)    \n",
        "    img_height=int(class_df['height'].iloc[0])\n",
        "    img_width =int(class_df['width'].iloc[0])\n",
        "    img_size=(img_width, img_height)\n",
        "    scale=class_df['scale by'].iloc[0] \n",
        "    try: \n",
        "        s=int(scale)\n",
        "        s2=1\n",
        "        s1=0\n",
        "    except:\n",
        "        split=scale.split('-')\n",
        "        s1=float(split[1])\n",
        "        s2=float(split[0].split('*')[1]) \n",
        "        print (s1,s2)\n",
        "    path_list=[]\n",
        "    paths=os.listdir(sdir)\n",
        "    for f in paths:\n",
        "        path_list.append(os.path.join(sdir,f))\n",
        "    print (' Model is being loaded- this will take about 10 seconds')\n",
        "    model=load_model(model_path)\n",
        "    image_count=len(path_list)    \n",
        "    index_list=[] \n",
        "    prob_list=[]\n",
        "    cropped_image_list=[]\n",
        "    good_image_count=0\n",
        "    for i in range (image_count):       \n",
        "        img=cv2.imread(path_list[i])\n",
        "        if crop_image == True:\n",
        "            status, img=crop(img)\n",
        "        else:\n",
        "            status=True\n",
        "        if status== True:\n",
        "            good_image_count +=1\n",
        "            img=cv2.resize(img, img_size)            \n",
        "            cropped_image_list.append(img)\n",
        "            img=img*s2 - s1\n",
        "            img=np.expand_dims(img, axis=0)\n",
        "            p= np.squeeze (model.predict(img))           \n",
        "            index=np.argmax(p)            \n",
        "            prob=p[index]\n",
        "            index_list.append(index)\n",
        "            prob_list.append(prob)\n",
        "    if good_image_count==1:\n",
        "        class_name= class_df['class'].iloc[index_list[0]]\n",
        "        probability= prob_list[0]\n",
        "        img=cropped_image_list [0] \n",
        "        plt.title(class_name, color='blue', fontsize=16)\n",
        "        plt.axis('off')\n",
        "        plt.imshow(img)\n",
        "        return class_name, probability\n",
        "    elif good_image_count == 0:\n",
        "        return None, None\n",
        "    most=0\n",
        "    for i in range (len(index_list)-1):\n",
        "        key= index_list[i]\n",
        "        keycount=0\n",
        "        for j in range (i+1, len(index_list)):\n",
        "            nkey= index_list[j]            \n",
        "            if nkey == key:\n",
        "                keycount +=1                \n",
        "        if keycount> most:\n",
        "            most=keycount\n",
        "            isave=i             \n",
        "    best_index=index_list[isave]    \n",
        "    psum=0\n",
        "    bestsum=0\n",
        "    for i in range (len(index_list)):\n",
        "        psum += prob_list[i]\n",
        "        if index_list[i]==best_index:\n",
        "            bestsum += prob_list[i]  \n",
        "    img= cropped_image_list[isave]/255    \n",
        "    class_name=class_df['class'].iloc[best_index]\n",
        "    plt.title(class_name, color='blue', fontsize=16)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(img)\n",
        "    return class_name, bestsum/image_count"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-08-09T22:07:59.774791Z",
          "iopub.execute_input": "2021-08-09T22:07:59.77529Z",
          "iopub.status.idle": "2021-08-09T22:07:59.80164Z",
          "shell.execute_reply.started": "2021-08-09T22:07:59.775227Z",
          "shell.execute_reply": "2021-08-09T22:07:59.800545Z"
        },
        "trusted": true,
        "id": "LaRzz1qhS2IE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### define a function tha takes in a dataframe df, and integer max_size and a string column\n",
        "### and returns a dataframe where the number of samples for any class specified by column\n",
        "### is limited to max samples"
      ],
      "metadata": {
        "id": "LDhskVwhS2IE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trim (df, max_size, min_size, column):\n",
        "    df=df.copy()\n",
        "    sample_list=[] \n",
        "    groups=df.groupby(column)\n",
        "    for label in df[column].unique():        \n",
        "        group=groups.get_group(label)\n",
        "        sample_count=len(group)         \n",
        "        if sample_count> max_size :\n",
        "            samples=group.sample(max_size, replace=False, weights=None, random_state=123, axis=0).reset_index(drop=True)\n",
        "            sample_list.append(samples)\n",
        "        elif sample_count>= min_size:\n",
        "            sample_list.append(group)\n",
        "    df=pd.concat(sample_list, axis=0).reset_index(drop=True)\n",
        "    balance=list(df[column].value_counts())\n",
        "    print (balance)\n",
        "    return df"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-08-09T22:07:59.808476Z",
          "iopub.execute_input": "2021-08-09T22:07:59.809203Z",
          "iopub.status.idle": "2021-08-09T22:07:59.825143Z",
          "shell.execute_reply.started": "2021-08-09T22:07:59.809139Z",
          "shell.execute_reply": "2021-08-09T22:07:59.824101Z"
        },
        "trusted": true,
        "id": "495Kevr7S2IE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### input an image"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.018355,
          "end_time": "2021-05-21T20:58:45.477867",
          "exception": false,
          "start_time": "2021-05-21T20:58:45.459512",
          "status": "completed"
        },
        "tags": [],
        "id": "8uJXVWZCS2IE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fpath=r'../input/edible-and-poisonous-fungi/edible mushroom sporocarp/ce (1).jpeg'\n",
        "img=plt.imread(fpath)\n",
        "print (img.shape)\n",
        "imshow(img)\n"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.393288,
          "end_time": "2021-05-21T20:58:45.889884",
          "exception": false,
          "start_time": "2021-05-21T20:58:45.496596",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-08-09T22:07:59.842325Z",
          "iopub.execute_input": "2021-08-09T22:07:59.842694Z",
          "iopub.status.idle": "2021-08-09T22:08:00.239499Z",
          "shell.execute_reply.started": "2021-08-09T22:07:59.842657Z",
          "shell.execute_reply": "2021-08-09T22:08:00.237176Z"
        },
        "trusted": true,
        "id": "uGlGVAT8S2IF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### define function to preprocess the dataframe"
      ],
      "metadata": {
        "id": "30V0b5S7S2IF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess (sdir, trsplit, vsplit):\n",
        "    filepaths=[]\n",
        "    labels=[]    \n",
        "    classlist=os.listdir(sdir)\n",
        "    for klass in classlist:\n",
        "        classpath=os.path.join(sdir,klass)\n",
        "        flist=os.listdir(classpath)\n",
        "        for f in flist:\n",
        "            fpath=os.path.join(classpath,f)\n",
        "            filepaths.append(fpath)\n",
        "            labels.append(klass)\n",
        "    Fseries=pd.Series(filepaths, name='filepaths')\n",
        "    Lseries=pd.Series(labels, name='labels')\n",
        "    df=pd.concat([Fseries, Lseries], axis=1)       \n",
        "    # split df into train_df and test_df \n",
        "    dsplit=vsplit/(1-trsplit)\n",
        "    strat=df['labels']    \n",
        "    train_df, dummy_df=train_test_split(df, train_size=trsplit, shuffle=True, random_state=123, stratify=strat)\n",
        "    strat=dummy_df['labels']\n",
        "    valid_df, test_df=train_test_split(dummy_df, train_size=dsplit, shuffle=True, random_state=123, stratify=strat)\n",
        "    print('train_df length: ', len(train_df), '  test_df length: ',len(test_df), '  valid_df length: ', len(valid_df))\n",
        "    print(train_df['labels'].value_counts())\n",
        "    return train_df, test_df, valid_df\n",
        "    "
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-08-09T22:08:00.241299Z",
          "iopub.execute_input": "2021-08-09T22:08:00.241675Z",
          "iopub.status.idle": "2021-08-09T22:08:00.253268Z",
          "shell.execute_reply.started": "2021-08-09T22:08:00.241636Z",
          "shell.execute_reply": "2021-08-09T22:08:00.25232Z"
        },
        "trusted": true,
        "id": "D5SE3TkOS2IF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sdir=r'../input/edible-and-poisonous-fungi'\n",
        "train_df, test_df, valid_df= preprocess(sdir, .8,.1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-08-09T22:08:00.254788Z",
          "iopub.execute_input": "2021-08-09T22:08:00.25515Z",
          "iopub.status.idle": "2021-08-09T22:08:00.293019Z",
          "shell.execute_reply.started": "2021-08-09T22:08:00.25512Z",
          "shell.execute_reply": "2021-08-09T22:08:00.292147Z"
        },
        "trusted": true,
        "id": "8tnkGx0AS2IF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### train_df is   not balanced having a ange of 1087 to 373 image sample per class\n",
        "### limit maximum samples in a class to 688. Then create augmented images so each\n",
        "### class has 688 samples."
      ],
      "metadata": {
        "id": "V0vAeC9lS2IF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def balance(train_df,max_samples, min_samples, column, working_dir, image_size):\n",
        "    train_df=train_df.copy()\n",
        "    train_df=trim (train_df, max_samples, min_samples, column)    \n",
        "    # make directories to store augmented images\n",
        "    aug_dir=os.path.join(working_dir, 'aug')\n",
        "    if os.path.isdir(aug_dir):\n",
        "        shutil.rmtree(aug_dir)\n",
        "    os.mkdir(aug_dir)\n",
        "    for label in train_df['labels'].unique():    \n",
        "        dir_path=os.path.join(aug_dir,label)    \n",
        "        os.mkdir(dir_path)\n",
        "    # create and store the augmented images  \n",
        "    total=0\n",
        "    gen=ImageDataGenerator(horizontal_flip=True,  rotation_range=20, width_shift_range=.2,\n",
        "                                  height_shift_range=.2, zoom_range=.2)\n",
        "    groups=train_df.groupby('labels') # group by class\n",
        "    for label in train_df['labels'].unique():  # for every class               \n",
        "        group=groups.get_group(label)  # a dataframe holding only rows with the specified label \n",
        "        sample_count=len(group)   # determine how many samples there are in this class  \n",
        "        if sample_count< max_samples: # if the class has less than target number of images\n",
        "            aug_img_count=0\n",
        "            delta=max_samples-sample_count  # number of augmented images to create\n",
        "            target_dir=os.path.join(aug_dir, label)  # define where to write the images    \n",
        "            aug_gen=gen.flow_from_dataframe( group,  x_col='filepaths', y_col=None, target_size=image_size,\n",
        "                                            class_mode=None, batch_size=1, shuffle=False, \n",
        "                                            save_to_dir=target_dir, save_prefix='aug-', color_mode='rgb',\n",
        "                                            save_format='jpg')\n",
        "            while aug_img_count<delta:\n",
        "                images=next(aug_gen)            \n",
        "                aug_img_count += len(images)\n",
        "            total +=aug_img_count\n",
        "    print('Total Augmented images created= ', total)\n",
        "    # create aug_df and merge with train_df to create composite training set ndf\n",
        "    if total>0:\n",
        "        aug_fpaths=[]\n",
        "        aug_labels=[]\n",
        "        classlist=os.listdir(aug_dir)\n",
        "        for klass in classlist:\n",
        "            classpath=os.path.join(aug_dir, klass)     \n",
        "            flist=os.listdir(classpath)    \n",
        "            for f in flist:        \n",
        "                fpath=os.path.join(classpath,f)         \n",
        "                aug_fpaths.append(fpath)\n",
        "                aug_labels.append(klass)\n",
        "        Fseries=pd.Series(aug_fpaths, name='filepaths')\n",
        "        Lseries=pd.Series(aug_labels, name='labels')\n",
        "        aug_df=pd.concat([Fseries, Lseries], axis=1)\n",
        "        ndf=pd.concat([train_df,aug_df], axis=0).reset_index(drop=True)\n",
        "    else:\n",
        "        ndf=train_df\n",
        "    print (list(ndf['labels'].value_counts()) )\n",
        "    return ndf "
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-08-09T22:08:00.295556Z",
          "iopub.execute_input": "2021-08-09T22:08:00.295879Z",
          "iopub.status.idle": "2021-08-09T22:08:00.311424Z",
          "shell.execute_reply.started": "2021-08-09T22:08:00.295844Z",
          "shell.execute_reply": "2021-08-09T22:08:00.31045Z"
        },
        "trusted": true,
        "id": "o-JriI8WS2IF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_samples= 688\n",
        "min_samples=0\n",
        "column='labels'\n",
        "working_dir = r'./'\n",
        "img_size=(300, 500)\n",
        "ndf=balance(train_df,max_samples, min_samples, column, working_dir, img_size)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-08-09T22:08:00.313096Z",
          "iopub.execute_input": "2021-08-09T22:08:00.313574Z",
          "iopub.status.idle": "2021-08-09T22:08:16.766196Z",
          "shell.execute_reply.started": "2021-08-09T22:08:00.313511Z",
          "shell.execute_reply": "2021-08-09T22:08:16.765364Z"
        },
        "trusted": true,
        "id": "88YFbJvRS2IG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The ndf dataframe is now balanced with 688 samples per class"
      ],
      "metadata": {
        "id": "B9y93AUAS2IG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### create train, test and validation generators"
      ],
      "metadata": {
        "id": "vXjqUW4nS2IG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "channels=3\n",
        "batch_size=30\n",
        "img_shape=(img_size[0], img_size[1], channels)\n",
        "length=len(test_df)\n",
        "test_batch_size=sorted([int(length/n) for n in range(1,length+1) if length % n ==0 and length/n<=80],reverse=True)[0]  \n",
        "test_steps=int(length/test_batch_size)\n",
        "print ( 'test batch size: ' ,test_batch_size, '  test steps: ', test_steps)\n",
        "def scalar(img):    \n",
        "    return img  # EfficientNet expects pixelsin range 0 to 255 so no scaling is required\n",
        "trgen=ImageDataGenerator(preprocessing_function=scalar, horizontal_flip=True)\n",
        "tvgen=ImageDataGenerator(preprocessing_function=scalar)\n",
        "train_gen=trgen.flow_from_dataframe( ndf, x_col='filepaths', y_col='labels', target_size=img_size, class_mode='categorical',\n",
        "                                    color_mode='rgb', shuffle=True, batch_size=batch_size)\n",
        "test_gen=tvgen.flow_from_dataframe( test_df, x_col='filepaths', y_col='labels', target_size=img_size, class_mode='categorical',\n",
        "                                    color_mode='rgb', shuffle=False, batch_size=test_batch_size)\n",
        "\n",
        "valid_gen=tvgen.flow_from_dataframe( valid_df, x_col='filepaths', y_col='labels', target_size=img_size, class_mode='categorical',\n",
        "                                    color_mode='rgb', shuffle=True, batch_size=batch_size)\n",
        "classes=list(train_gen.class_indices.keys())\n",
        "class_count=len(classes)\n",
        "train_steps=int(np.ceil(len(train_gen.labels)/batch_size))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-08-09T22:08:16.767494Z",
          "iopub.execute_input": "2021-08-09T22:08:16.76785Z",
          "iopub.status.idle": "2021-08-09T22:08:17.332409Z",
          "shell.execute_reply.started": "2021-08-09T22:08:16.767811Z",
          "shell.execute_reply": "2021-08-09T22:08:17.331Z"
        },
        "trusted": true,
        "id": "EytjHvCUS2IG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_image_samples(train_gen)\n"
      ],
      "metadata": {
        "papermill": {
          "duration": 3.031821,
          "end_time": "2021-05-21T21:00:01.820151",
          "exception": false,
          "start_time": "2021-05-21T20:59:58.78833",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-08-09T22:08:17.333657Z",
          "iopub.execute_input": "2021-08-09T22:08:17.334009Z",
          "iopub.status.idle": "2021-08-09T22:08:20.401191Z",
          "shell.execute_reply.started": "2021-08-09T22:08:17.33397Z",
          "shell.execute_reply": "2021-08-09T22:08:20.40029Z"
        },
        "trusted": true,
        "id": "wLJikYj9S2IG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### create the model"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.067478,
          "end_time": "2021-05-21T21:00:02.234057",
          "exception": false,
          "start_time": "2021-05-21T21:00:02.166579",
          "status": "completed"
        },
        "tags": [],
        "id": "SQ3X5QmiS2IG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name='EfficientNetB3'\n",
        "base_model=tf.keras.applications.EfficientNetB2(include_top=False, weights=\"imagenet\",input_shape=img_shape, pooling='max') \n",
        "x=base_model.output\n",
        "x=keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001 )(x)\n",
        "x = Dense(256, kernel_regularizer = regularizers.l2(l = 0.016),activity_regularizer=regularizers.l1(0.006),\n",
        "                bias_regularizer=regularizers.l1(0.006) ,activation='relu')(x)\n",
        "x=Dropout(rate=.45, seed=123)(x)        \n",
        "output=Dense(class_count, activation='softmax')(x)\n",
        "model=Model(inputs=base_model.input, outputs=output)\n",
        "model.compile(Adamax(lr=.001), loss='categorical_crossentropy', metrics=['accuracy']) "
      ],
      "metadata": {
        "papermill": {
          "duration": 5.559742,
          "end_time": "2021-05-21T21:00:07.864272",
          "exception": false,
          "start_time": "2021-05-21T21:00:02.30453",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-08-09T22:08:20.403913Z",
          "iopub.execute_input": "2021-08-09T22:08:20.404463Z",
          "iopub.status.idle": "2021-08-09T22:08:23.275708Z",
          "shell.execute_reply.started": "2021-08-09T22:08:20.40442Z",
          "shell.execute_reply": "2021-08-09T22:08:23.2748Z"
        },
        "trusted": true,
        "id": "dkdNvy1ZS2IG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### instantiate the custom callback and train the model"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.068299,
          "end_time": "2021-05-21T21:00:08.302663",
          "exception": false,
          "start_time": "2021-05-21T21:00:08.234364",
          "status": "completed"
        },
        "tags": [],
        "id": "-arkJQQyS2IG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs =40\n",
        "patience= 1 # number of epochs to wait to adjust lr if monitored value does not improve\n",
        "stop_patience =3 # number of epochs to wait before stopping training if monitored value does not improve\n",
        "threshold=.9 # if train accuracy is < threshhold adjust monitor accuracy, else monitor validation loss\n",
        "factor=.5 # factor to reduce lr by\n",
        "dwell=True # experimental, if True and monitored metric does not improve on current epoch set  modelweights back to weights of previous epoch\n",
        "freeze=False # if true free weights of  the base model\n",
        "ask_epoch=5 # number of epochs to run before asking if you want to halt training\n",
        "batches=train_steps\n",
        "callbacks=[LRA(model=model,base_model= base_model,patience=patience,stop_patience=stop_patience, threshold=threshold,\n",
        "                   factor=factor,dwell=dwell, batches=batches,initial_epoch=0,epochs=epochs, ask_epoch=ask_epoch )]\n",
        "\n",
        "history=model.fit(x=train_gen,  epochs=epochs, verbose=0, callbacks=callbacks,  validation_data=valid_gen,\n",
        "               validation_steps=None,  shuffle=False,  initial_epoch=0)"
      ],
      "metadata": {
        "papermill": {
          "duration": 4613.138096,
          "end_time": "2021-05-21T22:17:01.509111",
          "exception": false,
          "start_time": "2021-05-21T21:00:08.371015",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-08-09T22:08:23.277293Z",
          "iopub.execute_input": "2021-08-09T22:08:23.277611Z",
          "iopub.status.idle": "2021-08-09T22:39:27.578866Z",
          "shell.execute_reply.started": "2021-08-09T22:08:23.277579Z",
          "shell.execute_reply": "2021-08-09T22:39:27.577923Z"
        },
        "trusted": true,
        "id": "QrZgR17nS2IG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### evaluate model on the test set then save the model"
      ],
      "metadata": {
        "papermill": {
          "duration": 2.985903,
          "end_time": "2021-05-21T22:17:31.319926",
          "exception": false,
          "start_time": "2021-05-21T22:17:28.334023",
          "status": "completed"
        },
        "tags": [],
        "id": "pF0BreQLS2IH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tr_plot(history,0)\n",
        "subject='mushrooms'\n",
        "acc=model.evaluate( test_gen, batch_size=test_batch_size, verbose=1, steps=test_steps, return_dict=False)[1]*100\n",
        "msg=f'accuracy on the test set is {acc:5.2f} %'\n",
        "print_in_color(msg, (0,255,0),(55,65,80))\n",
        "generator=train_gen\n",
        "scale = 1\n",
        "model_save_loc, csv_save_loc=saver(working_dir, model, model_name, subject, acc, img_size, scale,  generator)"
      ],
      "metadata": {
        "papermill": {
          "duration": 14.533987,
          "end_time": "2021-05-21T22:17:49.515069",
          "exception": false,
          "start_time": "2021-05-21T22:17:34.981082",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-08-09T22:39:27.580602Z",
          "iopub.execute_input": "2021-08-09T22:39:27.581022Z",
          "iopub.status.idle": "2021-08-09T22:39:33.624726Z",
          "shell.execute_reply.started": "2021-08-09T22:39:27.580978Z",
          "shell.execute_reply": "2021-08-09T22:39:33.623925Z"
        },
        "trusted": true,
        "id": "p1cfVXCeS2IH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### make predictions on test set and generate confusion matrix and classification report"
      ],
      "metadata": {
        "papermill": {
          "duration": 3.155539,
          "end_time": "2021-05-21T22:17:55.624417",
          "exception": false,
          "start_time": "2021-05-21T22:17:52.468878",
          "status": "completed"
        },
        "tags": [],
        "id": "Yme8X5OwS2IH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_code=0\n",
        "preds=model.predict(test_gen, steps=test_steps, verbose=1) \n",
        "print_info( test_gen, preds, print_code, working_dir, subject )  "
      ],
      "metadata": {
        "papermill": {
          "duration": 8.948725,
          "end_time": "2021-05-21T22:18:07.482198",
          "exception": false,
          "start_time": "2021-05-21T22:17:58.533473",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-08-09T22:39:33.62594Z",
          "iopub.execute_input": "2021-08-09T22:39:33.626463Z",
          "iopub.status.idle": "2021-08-09T22:39:39.646137Z",
          "shell.execute_reply.started": "2021-08-09T22:39:33.626417Z",
          "shell.execute_reply": "2021-08-09T22:39:39.645161Z"
        },
        "trusted": true,
        "id": "zEuUV0OJS2IH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### to test the classifier kernel create a directory with a single image in it"
      ],
      "metadata": {
        "id": "jgKjt_XOS2IH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "store_path=os.path.join(working_dir, 'storage')\n",
        "if os.path.isdir(store_path):\n",
        "    shutil.rmtree(store_path)\n",
        "os.mkdir(store_path)\n",
        "# input an image of an edible sporocarp\n",
        "img_path=r'../input/edible-and-poisonous-fungi/edible sporocarp/ncvc (1).jpeg'\n",
        "img=cv2.imread(img_path,  cv2.IMREAD_REDUCED_COLOR_2)\n",
        "img=cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "# model was trained on rgb images so convert image to rgb\n",
        "file_name=os.path.split(img_path)[1]\n",
        "dst_path=os.path.join(store_path, file_name)\n",
        "cv2.imwrite(dst_path, img)\n",
        "# check if the directory was created and image stored\n",
        "print (os.listdir(store_path))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-08-09T22:39:39.647506Z",
          "iopub.execute_input": "2021-08-09T22:39:39.647853Z",
          "iopub.status.idle": "2021-08-09T22:39:39.781114Z",
          "shell.execute_reply.started": "2021-08-09T22:39:39.647815Z",
          "shell.execute_reply": "2021-08-09T22:39:39.780189Z"
        },
        "trusted": true,
        "id": "si-D7ocnS2IH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### use the predictor function to classify the single image"
      ],
      "metadata": {
        "id": "pw7D0RQ0S2IH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_path=csv_save_loc # path to class_dict.csv\n",
        "model_path=model_save_loc # path to the trained model\n",
        "class_name, probability=predictor(store_path, csv_path,  model_path, crop_image = False) # run the classifier\n",
        "msg=f' image is of class {class_name} with a probability of {probability * 100: 6.2f} %'\n",
        "print_in_color(msg, (0,255,255), (65,85,55))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-08-09T22:39:39.782461Z",
          "iopub.execute_input": "2021-08-09T22:39:39.783008Z",
          "iopub.status.idle": "2021-08-09T22:39:46.111623Z",
          "shell.execute_reply.started": "2021-08-09T22:39:39.78297Z",
          "shell.execute_reply": "2021-08-09T22:39:46.110738Z"
        },
        "trusted": true,
        "id": "YvGxaoKIS2IH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's see how we do on F1 score if we do not balance the dataset"
      ],
      "metadata": {
        "id": "yUEYJF4LS2IH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### recalculate train_df, test_df and valid_df but do not balance the dataset"
      ],
      "metadata": {
        "id": "sd1G-P0tS2IH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sdir=r'../input/edible-and-poisonous-fungi'\n",
        "train_df, test_df, valid_df= preprocess(sdir, .8,.1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-08-09T22:39:46.112986Z",
          "iopub.execute_input": "2021-08-09T22:39:46.113357Z",
          "iopub.status.idle": "2021-08-09T22:39:46.147742Z",
          "shell.execute_reply.started": "2021-08-09T22:39:46.113318Z",
          "shell.execute_reply": "2021-08-09T22:39:46.146772Z"
        },
        "trusted": true,
        "id": "OjWmSDiVS2II"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### recreate the generators"
      ],
      "metadata": {
        "id": "q09Ij5aSS2II"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "channels=3\n",
        "batch_size=30\n",
        "img_shape=(img_size[0], img_size[1], channels)\n",
        "length=len(test_df)\n",
        "test_batch_size=sorted([int(length/n) for n in range(1,length+1) if length % n ==0 and length/n<=80],reverse=True)[0]  \n",
        "test_steps=int(length/test_batch_size)\n",
        "print ( 'test batch size: ' ,test_batch_size, '  test steps: ', test_steps)\n",
        "def scalar(img):    \n",
        "    return img  # EfficientNet expects pixelsin range 0 to 255 so no scaling is required\n",
        "trgen=ImageDataGenerator(preprocessing_function=scalar, horizontal_flip=True)\n",
        "tvgen=ImageDataGenerator(preprocessing_function=scalar)\n",
        "train_gen=trgen.flow_from_dataframe( train_df, x_col='filepaths', y_col='labels', target_size=img_size, class_mode='categorical',\n",
        "                                    color_mode='rgb', shuffle=True, batch_size=batch_size)\n",
        "test_gen=tvgen.flow_from_dataframe( test_df, x_col='filepaths', y_col='labels', target_size=img_size, class_mode='categorical',\n",
        "                                    color_mode='rgb', shuffle=False, batch_size=test_batch_size)\n",
        "\n",
        "valid_gen=tvgen.flow_from_dataframe( valid_df, x_col='filepaths', y_col='labels', target_size=img_size, class_mode='categorical',\n",
        "                                    color_mode='rgb', shuffle=True, batch_size=batch_size)\n",
        "classes=list(train_gen.class_indices.keys())\n",
        "class_count=len(classes)\n",
        "train_steps=int(np.ceil(len(train_gen.labels)/batch_size))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-08-09T22:39:46.149208Z",
          "iopub.execute_input": "2021-08-09T22:39:46.149584Z",
          "iopub.status.idle": "2021-08-09T22:39:47.65404Z",
          "shell.execute_reply.started": "2021-08-09T22:39:46.149545Z",
          "shell.execute_reply": "2021-08-09T22:39:47.653174Z"
        },
        "trusted": true,
        "id": "eaPFDCTDS2II"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### recreate the model"
      ],
      "metadata": {
        "id": "71ds2vp8S2II"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name='EfficientNetB3'\n",
        "base_model=tf.keras.applications.EfficientNetB2(include_top=False, weights=\"imagenet\",input_shape=img_shape, pooling='max') \n",
        "x=base_model.output\n",
        "x=keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001 )(x)\n",
        "x = Dense(256, kernel_regularizer = regularizers.l2(l = 0.016),activity_regularizer=regularizers.l1(0.006),\n",
        "                bias_regularizer=regularizers.l1(0.006) ,activation='relu')(x)\n",
        "x=Dropout(rate=.45, seed=123)(x)        \n",
        "output=Dense(class_count, activation='softmax')(x)\n",
        "model=Model(inputs=base_model.input, outputs=output)\n",
        "model.compile(Adamax(lr=.001), loss='categorical_crossentropy', metrics=['accuracy']) "
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-08-09T22:39:47.657085Z",
          "iopub.execute_input": "2021-08-09T22:39:47.657369Z",
          "iopub.status.idle": "2021-08-09T22:39:50.027802Z",
          "shell.execute_reply.started": "2021-08-09T22:39:47.657339Z",
          "shell.execute_reply": "2021-08-09T22:39:50.02686Z"
        },
        "trusted": true,
        "id": "vXJNzH-iS2II"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### train model on unbalanced dataset"
      ],
      "metadata": {
        "id": "u3ycOnvNS2II"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()\n",
        "epochs =40\n",
        "patience= 1 # number of epochs to wait to adjust lr if monitored value does not improve\n",
        "stop_patience =3 # number of epochs to wait before stopping training if monitored value does not improve\n",
        "threshold=.9 # if train accuracy is < threshhold adjust monitor accuracy, else monitor validation loss\n",
        "factor=.5 # factor to reduce lr by\n",
        "dwell=True # experimental, if True and monitored metric does not improve on current epoch set  modelweights back to weights of previous epoch\n",
        "freeze=False # if true free weights of  the base model\n",
        "ask_epoch=5 # number of epochs to run before asking if you want to halt training\n",
        "batches=train_steps\n",
        "callbacks=[LRA(model=model,base_model= base_model,patience=patience,stop_patience=stop_patience, threshold=threshold,\n",
        "                   factor=factor,dwell=dwell, batches=batches,initial_epoch=0,epochs=epochs, ask_epoch=ask_epoch )]\n",
        "\n",
        "history=model.fit(x=train_gen,  epochs=epochs, verbose=0, callbacks=callbacks,  validation_data=valid_gen,\n",
        "               validation_steps=None,  shuffle=False,  initial_epoch=0)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-08-09T22:39:50.029085Z",
          "iopub.execute_input": "2021-08-09T22:39:50.029462Z",
          "iopub.status.idle": "2021-08-09T22:54:58.852089Z",
          "shell.execute_reply.started": "2021-08-09T22:39:50.029417Z",
          "shell.execute_reply": "2021-08-09T22:54:58.851313Z"
        },
        "trusted": true,
        "id": "GSOsZQ1lS2II"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### plot training data and evaluate model accuracy on test set"
      ],
      "metadata": {
        "id": "xPyq4ehUS2II"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tr_plot(history,0)\n",
        "subject='mushrooms'\n",
        "acc=model.evaluate( test_gen, batch_size=test_batch_size, verbose=1, steps=test_steps, return_dict=False)[1]*100\n",
        "msg=f'accuracy on the test set is {acc:5.2f} %'\n",
        "print_in_color(msg, (0,255,0),(55,65,80))\n",
        "generator=train_gen\n",
        "scale = 1\n",
        "model_save_loc, csv_save_loc=saver(working_dir, model, model_name, subject, acc, img_size, scale,  generator)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-08-09T22:54:58.854775Z",
          "iopub.execute_input": "2021-08-09T22:54:58.855054Z",
          "iopub.status.idle": "2021-08-09T22:55:04.156856Z",
          "shell.execute_reply.started": "2021-08-09T22:54:58.855028Z",
          "shell.execute_reply": "2021-08-09T22:55:04.156081Z"
        },
        "trusted": true,
        "id": "O3HqEc4tS2II"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### make predictions and calculate F1 score for unbalanced data"
      ],
      "metadata": {
        "id": "H7KGClHVS2IJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_code=0\n",
        "preds=model.predict(test_gen) \n",
        "print_info( test_gen, preds, print_code, working_dir, subject )  "
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-08-09T22:55:04.158314Z",
          "iopub.execute_input": "2021-08-09T22:55:04.158663Z",
          "iopub.status.idle": "2021-08-09T22:55:10.058553Z",
          "shell.execute_reply.started": "2021-08-09T22:55:04.158625Z",
          "shell.execute_reply": "2021-08-09T22:55:10.057673Z"
        },
        "trusted": true,
        "id": "uAJ9W6ppS2IJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}