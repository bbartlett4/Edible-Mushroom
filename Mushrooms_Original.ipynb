{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mushrooms-Original.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNaJ1HH4I/ZOyHbkjsKwztf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bbartlett4/Edible-Mushroom/blob/main/Mushrooms_Original.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "PElToHcubmZn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_hub as hub \n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from glob import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import drive\n",
        "import shutil "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def show_image(file_path):\n",
        "    img = tf.keras.preprocessing.image.load_img(file_path, target_size=(228, 228))\n",
        "    plt.imshow(img)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "4-nN6cfXIzdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image(item):\n",
        "    image_string = tf.io.read_file(item[0])\n",
        "    image_decoded = tf.image.decode_jpeg(image_string, channels=3)\n",
        "    image_resized = tf.image.resize(image_decoded, (IMAGE_SIZE, IMAGE_SIZE))\n",
        "    image_resized = tf.cast(image_resized, tf.float32) / 255.0\n",
        "    return image_resized, tf.strings.to_number(item[1], tf.int64)"
      ],
      "metadata": {
        "id": "_UvYSUVAI0s_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset(edible_fungies, poisonous_fungies, mode, batch_size):\n",
        "    x = list(edible_fungies) + list(poisonous_fungies)\n",
        "    y = [0] * len(edible_fungies) + [1] * len(poisonous_fungies)\n",
        "    items = [(a, b) for (a, b) in zip(x, y)]\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(np.array(items)).shuffle(len(x))\n",
        "    dataset = dataset.map(preprocess_image).batch(batch_size)\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "BIQ54I6dI3Py"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_balanced_dataset(edible_fungies, poisonous_fungies, batch_count, batch_size, mode=\"train\"):\n",
        "    length_per_category = batch_size * batch_count // 2\n",
        "    edible_indices = np.random.choice(len(edible_fungies), length_per_category)\n",
        "    poisonous_indices = np.random.choice(len(poisonous_fungies), length_per_category)\n",
        "    samle_count = 2 * length_per_category\n",
        "    return get_dataset(\n",
        "        edible_fungies[edible_indices], \n",
        "        poisonous_fungies[poisonous_indices], \n",
        "        mode, \n",
        "        batch_size\n",
        "    ), samle_count"
      ],
      "metadata": {
        "id": "XMECvf3CI4VD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Only Run to insert Data First Time"
      ],
      "metadata": {
        "id": "FJHm8S_UKXOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lables = [\"edible\", \"poisonous\"]\n",
        "directory_group = [\n",
        "    ['edible mushroom sporocarp', 'edible sporocarp'], \n",
        "    ['poisonous mushroom sporocarp', 'poisonous sporocarp']\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "import shutil \n",
        "drive.mount('/content/drive')\n",
        "!unzip \"/content/drive/My Drive/archive.zip\" -d \"/content/drive/My Drive/Colab Notebooks/Project Data/\"\n",
        "\n",
        "# feel free to edit this cell to work with your local or colab directory\n",
        "WORKING_DIR = '/content/drive/My Drive/Colab Notebooks/Project Data'\n",
        "TRAIN_DIR = '/content/drive/My Drive/Colab Notebooks/Project Data/train'\n",
        "VAL_DIR = '/content/drive/My Drive/Colab Notebooks/Project Data/val'\n",
        "\n",
        "shutil.move(os.path.join(WORKING_DIR,'edible mushroom sporocarp'), os.path.join(TRAIN_DIR, 'edible mushroom sporocarp'))\n",
        "shutil.move(os.path.join(WORKING_DIR,'edible sporocarp'), os.path.join(TRAIN_DIR, 'Cube'))\n",
        "shutil.move(os.path.join(WORKING_DIR,'poisonous mushroom sporocarp'), os.path.join(TRAIN_DIR, 'poisonous mushroom sporocarp'))\n",
        "shutil.move(os.path.join(WORKING_DIR,'poisonous sporocarp'), os.path.join(TRAIN_DIR, 'poisonous sporocarp'))\n"
      ],
      "metadata": {
        "id": "-FPCefzoI9s1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EdibleMushroom = glob(TRAIN_DIR + '/edible mushroom sporocarp/*.jpg')\n",
        "EdibleSporo = glob(TRAIN_DIR + '/edible sporocarp/*.jpg')\n",
        "PoisonMushroom = glob(TRAIN_DIR + '/poisonous mushroom sporocarp/*.jpg')\n",
        "PoisonSporo = glob(TRAIN_DIR + '/poisonous sporocarp/*.jpg')\n",
        "edible_fungies = list(set(EdibleMushroom))\n",
        "poisonous_fungies = list(set(PoisonMushroom))"
      ],
      "metadata": {
        "id": "zlUC4oICI54Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "validation_split = 0.2\n",
        "edible_fungies_split_index = int((1 - validation_split) * len(edible_fungies))\n",
        "poisonous_fungies_split_index = int((1 - validation_split) * len(poisonous_fungies))\n",
        "train_edible_fungies, valid_edible_fungies = edible_fungies[:edible_fungies_split_index],  edible_fungies[edible_fungies_split_index:] \n",
        "train_poisonous_fungies, valid_poisonous_fungies = poisonous_fungies[:poisonous_fungies_split_index],  poisonous_fungies[poisonous_fungies_split_index:] \n",
        "print(len(train_edible_fungies), len(valid_edible_fungies))\n",
        "print(len(train_poisonous_fungies), len(valid_poisonous_fungies))\n",
        "num_batch_per_epoch = min(len(train_edible_fungies), len(train_poisonous_fungies)) // batch_size\n",
        "print(num_batch_per_epoch)\n",
        "num_epochs = 50\n",
        "train_edible_fungies = np.array(train_edible_fungies)\n",
        "valid_edible_fungies = np.array(valid_edible_fungies)\n",
        "train_poisonous_fungies = np.array(train_poisonous_fungies)\n",
        "valid_poisonous_fungies = np.array(valid_poisonous_fungies)\n",
        "total_valid_count = len(valid_edible_fungies) + len(valid_poisonous_fungies)"
      ],
      "metadata": {
        "id": "BLrWzmGnJAZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(edible_fungies)"
      ],
      "metadata": {
        "id": "dSMYD0NxJCAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(poisonous_fungies)"
      ],
      "metadata": {
        "id": "cBP0DW4yJEeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "edible_fungies[:10]"
      ],
      "metadata": {
        "id": "mpS_pSYqJGGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "poisonous_fungies[:10]"
      ],
      "metadata": {
        "id": "RRkV-rX1JGpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    show_image(poisonous_fungies[np.random.randint(len(poisonous_fungies))])"
      ],
      "metadata": {
        "id": "uYu73UXZJJL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    show_image(edible_fungies[np.random.randint(len(edible_fungies))])"
      ],
      "metadata": {
        "id": "tmjJFsw0JKgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE = 224\n",
        "handle_base = \"mobilenet_v2\"\n",
        "MODULE_HANDLE =\"https://tfhub.dev/google/tf2-preview/{}/feature_vector/4\".format(handle_base)\n",
        "feature_extractor = hub.KerasLayer(MODULE_HANDLE,\n",
        "                                    input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
        "feature_extractor.trainable = False  "
      ],
      "metadata": {
        "id": "jwMgGA2xJN3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()\n",
        "model = tf.keras.Sequential([\n",
        "    feature_extractor,\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(2, activation='softmax')\n",
        "])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "-cihpU70JOUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_dataset = get_dataset(valid_edible_fungies, valid_poisonous_fungies, \"valid\", batch_size)\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "history = {\n",
        "    \"train_loss\": [],\n",
        "    \"valid_loss\": [],\n",
        "    \"train_accuracy\": [],\n",
        "    \"valid_accuracy\": []\n",
        "}\n",
        "for epoch in range(num_epochs):\n",
        "    begin_time = time.time()\n",
        "    train_dataset, total_train_count = get_balanced_dataset(train_edible_fungies, train_poisonous_fungies, num_batch_per_epoch, batch_size, mode=\"train\")\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "    correct_count = 0\n",
        "    total_count = 0\n",
        "    for (x_batch, y_true) in train_dataset:\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = model(x_batch)\n",
        "            predict_labels = tf.argmax(y_pred, axis=-1)\n",
        "            loss_value = loss(y_true, y_pred)\n",
        "        gradients = tape.gradient(loss_value, model.trainable_weights)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
        "        train_losses.append(loss_value)\n",
        "        correct_count += tf.reduce_sum(tf.cast(y_true == predict_labels, tf.int64))\n",
        "        total_count += y_true.shape[0]\n",
        "    train_loss = tf.reduce_mean(train_losses)\n",
        "    train_accuracy = correct_count / total_train_count\n",
        "    history[\"train_loss\"].append(train_loss)\n",
        "    history[\"train_accuracy\"].append(train_accuracy)\n",
        "    correct_count = 0\n",
        "    total_count = 0\n",
        "    for (x_batch, y_true) in valid_dataset:\n",
        "        y_pred = model(x_batch)\n",
        "        predict_labels = tf.argmax(y_pred, axis=-1)\n",
        "        loss_value = loss(y_true, y_pred)\n",
        "        valid_losses.append(loss_value)\n",
        "        correct_count += tf.reduce_sum(tf.cast(y_true == predict_labels, tf.int64))\n",
        "        total_count += y_true.shape[0]\n",
        "    valid_loss = tf.reduce_mean(valid_losses)\n",
        "    valid_accuracy = correct_count / total_valid_count\n",
        "    history[\"valid_loss\"].append(valid_loss)\n",
        "    history[\"valid_accuracy\"].append(valid_accuracy)\n",
        "    elapsed_time = time.time() -  begin_time\n",
        "    print(\"Epoch: %d / %d\"%(epoch + 1, num_epochs))\n",
        "    print(\"%.2fs Loss: %.2f Accuracy: %.2f Validation Loss: %.2f Validation Accuracy: %.2f\"%(elapsed_time, train_loss, train_accuracy, valid_loss, valid_accuracy))\n",
        "for key in history:\n",
        "    history[key] = list(np.array(history[key]))"
      ],
      "metadata": {
        "id": "sMERjdeAJR1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(history).plot()"
      ],
      "metadata": {
        "id": "G9JBzL2qJSmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_labels = []\n",
        "actual_labels = []\n",
        "for (x_batch, y_true) in valid_dataset:\n",
        "    y_pred = model(x_batch)\n",
        "    predicted_labels += list(np.array(tf.argmax(y_pred, axis=-1)))\n",
        "    actual_labels += list(np.array(y_true))"
      ],
      "metadata": {
        "id": "F6Br__wjJU4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matrix = confusion_matrix(actual_labels, predicted_labels)\n",
        "print(matrix)\n",
        "sns.heatmap(matrix)"
      ],
      "metadata": {
        "id": "ioy9WcBOJXai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cls_report = classification_report(predicted_labels, actual_labels)\n",
        "print(cls_report)"
      ],
      "metadata": {
        "id": "gr9hlJm2JYok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"model.h5\")"
      ],
      "metadata": {
        "id": "i6oii17QJZxw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}